---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a second-year Master's student in **Software Engineering** at **Peking University**. Prior to this, I obtained my Bachelor's degree from **Wuhan University**.

My research interests include **Large Language Models (LLMs)** and **Vision-Language Models (VLMs)**. Recently, I have been focusing on enhancing the reasoning capabilities of LLMs and VLMs, as well as optimizing tool-calling capabilities and accuracy.

# üî• News
- *2026.1*: &nbsp;üéâüéâ Our paper *"Accurate and Efficient Personalized Query Rewriting in Baidu Search"* was accepted to **WWW 2026**!

- *2025.5*: &nbsp;üéâüéâ Our paper *"Domain$o1$s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains"* was accepted to **ACL 2025 Findings**! 

# üìù Selected Publications 
- Accurate and Efficient Personalized Query Rewriting in Baidu Search ![CCF A](https://img.shields.io/badge/CCF-A-red?style=flat-square) ![WWW](https://img.shields.io/badge/WWW-2026-blue?style=flat-square)

  **Xu Chu**, Angela Li, Jiaming Zhang, Wei Li, Zhijie Tan, Dawei Yin, Shuaiqiang Wang, Daiting Shi

  *In Proc. of The 35th International World Wide Web Conference (**WWW**).*

- [Domain$o1$s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains](https://arxiv.org/abs/2501.14431) ![CCF A](https://img.shields.io/badge/CCF-A-red?style=flat-square) ![ACL](https://img.shields.io/badge/ACL-2025-blue?style=flat-square)  

  **Xu Chu**\*, Zhijie Tan\*, Hanlin Xue, Guanyu Wang, Tong Mo, Weiping Li

  *In Proc. of The 63rd Annual Meeting of the Association for Computational Linguistics (**ACL**).*

- [Adaptive Spatiotemporal Augmentation for Improving Dynamic Graph Learning](https://arxiv.org/abs/2501.10010) ![CCF B](https://img.shields.io/badge/CCF-B-green?style=flat-square) ![ICASSP](https://img.shields.io/badge/ICASSP-2025-blue?style=flat-square)  

  **Xu Chu**\*, Hanlin Xue\*, Bingce Wang, Xiaoyang Liu, Weiping Li, Tong Mo, Tuoyu Feng, Zhijie Tan

  *In Proc. of ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**).*
  
# ‚úèÔ∏è Preprints
- [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)

  **Xu Chu**\*, Xinrong Chen\*, Guanyu Wang\*, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li

- [GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand Graphs Better](https://arxiv.org/abs/2501.14427)

  **Xu Chu**\*, Hanlin Xue\*, Zhijie Tan, Bingce Wang, Tong Mo, Weiping Li

- [Order matters: Exploring order sensitivity in multimodal large language models](https://arxiv.org/abs/2410.16983)
  
  Zhijie Tan\*, **Xu Chu**\*, Weiping Li, Tong Mo

# üìñ Educations
- *2023.09 ‚Äì Present*: Master's student in Software Engineering under the supervision of Prof. Weiping Li at **Peking University**.
- *2019.09 ‚Äì 2023.07*: Bachelor of Engineering, School of Electronic Information, **Wuhan University**.

# üíª Internships
- *2024.01 ‚Äì 2024.05*, [NIO Shanghai](https://www.nio.cn/), China.  
  Contributed to the development of the LLM framework **BI Agent**, primarily responsible for intent understanding, task routing, and reasoning using LLMs.  

- *2025.04 ‚Äì 2026.01*, [Baidu](https://ir.baidu.com/company-overview), China.  
  During the summer internship, I received the Xinghai Talent Program. I participated in the pre-training of **Search LLM** and the reinforcement learning post-training of **Query Rewriting LLM**. My main focus was on compressing LLM chain-of-thought to reduce the inference time of the query rewriting LLM.

- *2026.01 ‚Äì present*, [Xiaomi](https://www.mi.com/about), China.  
  I received the **Top Campus Talent Program** from **Xiaomi's LLM team** during the fall recruitment. I will participate in optimizing the reinforcement learning training strategies for **DeepResearch** and **GUI Agent** during my internship and full-time employment.
